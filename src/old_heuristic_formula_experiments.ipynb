{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtext\n",
    "import torchtext.vocab as vocab\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import autocast, GradScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:4\") # TODO change to \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataset\n",
    "# Read the CSV file\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Splitting data into features and labels\n",
    "X = data['tweet'].values\n",
    "\n",
    "# Splitting the dataset into training and validation sets\n",
    "X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)\n",
    "\n",
    "# Creating DataFrame for training and validation sets\n",
    "train_data = pd.DataFrame({'tweet': X_train})\n",
    "test_data = pd.DataFrame({'tweet': X_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @FunSizedYogi: @TheBlackVoice well how else...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Funny thing is....it's not just the people doi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @winkSOSA: \"@AintShitSweet__: \"@Rakwon_OGOD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Jbrendaro30 @ZGabrail @ramsin1995 @GabeEli8 @...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S/o that real bitch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19821</th>\n",
       "      <td>The last at-bat at Yankee Stadium. Thanks for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19822</th>\n",
       "      <td>@_bradleey LMFAOOOO yooo I lost my elevator pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19823</th>\n",
       "      <td>#porn,#android,#iphone,#ipad,#sex,#xxx, | #Ana...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19824</th>\n",
       "      <td>RT @JennyJohnsonHi5: Just when I thought Justi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19825</th>\n",
       "      <td>bitches ain&amp;#8217;t shit, and they ain&amp;#8217;t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19826 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   tweet\n",
       "0      RT @FunSizedYogi: @TheBlackVoice well how else...\n",
       "1      Funny thing is....it's not just the people doi...\n",
       "2      RT @winkSOSA: \"@AintShitSweet__: \"@Rakwon_OGOD...\n",
       "3      @Jbrendaro30 @ZGabrail @ramsin1995 @GabeEli8 @...\n",
       "4                                    S/o that real bitch\n",
       "...                                                  ...\n",
       "19821  The last at-bat at Yankee Stadium. Thanks for ...\n",
       "19822  @_bradleey LMFAOOOO yooo I lost my elevator pa...\n",
       "19823  #porn,#android,#iphone,#ipad,#sex,#xxx, | #Ana...\n",
       "19824  RT @JennyJohnsonHi5: Just when I thought Justi...\n",
       "19825  bitches ain&#8217;t shit, and they ain&#8217;t...\n",
       "\n",
       "[19826 rows x 1 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>934 8616\\ni got a missed call from yo bitch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @KINGTUNCHI_: Fucking with a bad bitch you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @eanahS__: @1inkkofrosess lol my credit ain...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @Maxin_Betha Wipe the cum out of them faggo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Niggas cheat on they bitch and don't expect no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4952</th>\n",
       "      <td>@GrizzboAdams @wyattnuckels haha ight nig calm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4953</th>\n",
       "      <td>When you see kids being bad &amp;amp; their parent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4954</th>\n",
       "      <td>This bitch done blew my high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4955</th>\n",
       "      <td>Fat Trel that niggah &amp;#128076;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4956</th>\n",
       "      <td>Neverrr, play me for stupid because I'm far fr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4957 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet\n",
       "0           934 8616\\ni got a missed call from yo bitch\n",
       "1     RT @KINGTUNCHI_: Fucking with a bad bitch you ...\n",
       "2     RT @eanahS__: @1inkkofrosess lol my credit ain...\n",
       "3     RT @Maxin_Betha Wipe the cum out of them faggo...\n",
       "4     Niggas cheat on they bitch and don't expect no...\n",
       "...                                                 ...\n",
       "4952  @GrizzboAdams @wyattnuckels haha ight nig calm...\n",
       "4953  When you see kids being bad &amp; their parent...\n",
       "4954                       This bitch done blew my high\n",
       "4955                     Fat Trel that niggah &#128076;\n",
       "4956  Neverrr, play me for stupid because I'm far fr...\n",
       "\n",
       "[4957 rows x 1 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Model\n",
    "\n",
    "# # Load tokenizer and model\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"facebook/roberta-hate-speech-dynabench-r4-target\")\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"facebook/roberta-hate-speech-dynabench-r4-target\")\n",
    "# # This one said non-hate to a lot of hate speech as far as I have tried.\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Hate-speech-CNERG/dehatebert-mono-english\")\n",
    "target_model = AutoModelForSequenceClassification.from_pretrained(\"Hate-speech-CNERG/dehatebert-mono-english\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>934 8616\\ni got a missed call from yo bitch</td>\n",
       "      <td>[0.5540313124656677, 0.4459686875343323]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @KINGTUNCHI_: Fucking with a bad bitch you ...</td>\n",
       "      <td>[0.37748023867607117, 0.6225197315216064]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @eanahS__: @1inkkofrosess lol my credit ain...</td>\n",
       "      <td>[0.9582731127738953, 0.041726816445589066]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @Maxin_Betha Wipe the cum out of them faggo...</td>\n",
       "      <td>[0.095273457467556, 0.904726505279541]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Niggas cheat on they bitch and don't expect no...</td>\n",
       "      <td>[0.11824338138103485, 0.881756603717804]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4952</th>\n",
       "      <td>@GrizzboAdams @wyattnuckels haha ight nig calm...</td>\n",
       "      <td>[0.09661741554737091, 0.9033825397491455]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4953</th>\n",
       "      <td>When you see kids being bad &amp;amp; their parent...</td>\n",
       "      <td>[0.3516906201839447, 0.6483093500137329]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4954</th>\n",
       "      <td>This bitch done blew my high</td>\n",
       "      <td>[0.21178992092609406, 0.7882100939750671]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4955</th>\n",
       "      <td>Fat Trel that niggah &amp;#128076;</td>\n",
       "      <td>[0.08707571029663086, 0.9129243493080139]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4956</th>\n",
       "      <td>Neverrr, play me for stupid because I'm far fr...</td>\n",
       "      <td>[0.21719162166118622, 0.7828083634376526]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4957 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet  \\\n",
       "0           934 8616\\ni got a missed call from yo bitch   \n",
       "1     RT @KINGTUNCHI_: Fucking with a bad bitch you ...   \n",
       "2     RT @eanahS__: @1inkkofrosess lol my credit ain...   \n",
       "3     RT @Maxin_Betha Wipe the cum out of them faggo...   \n",
       "4     Niggas cheat on they bitch and don't expect no...   \n",
       "...                                                 ...   \n",
       "4952  @GrizzboAdams @wyattnuckels haha ight nig calm...   \n",
       "4953  When you see kids being bad &amp; their parent...   \n",
       "4954                       This bitch done blew my high   \n",
       "4955                     Fat Trel that niggah &#128076;   \n",
       "4956  Neverrr, play me for stupid because I'm far fr...   \n",
       "\n",
       "                                           label  \n",
       "0       [0.5540313124656677, 0.4459686875343323]  \n",
       "1      [0.37748023867607117, 0.6225197315216064]  \n",
       "2     [0.9582731127738953, 0.041726816445589066]  \n",
       "3         [0.095273457467556, 0.904726505279541]  \n",
       "4       [0.11824338138103485, 0.881756603717804]  \n",
       "...                                          ...  \n",
       "4952   [0.09661741554737091, 0.9033825397491455]  \n",
       "4953    [0.3516906201839447, 0.6483093500137329]  \n",
       "4954   [0.21178992092609406, 0.7882100939750671]  \n",
       "4955   [0.08707571029663086, 0.9129243493080139]  \n",
       "4956   [0.21719162166118622, 0.7828083634376526]  \n",
       "\n",
       "[4957 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For validation w.r.t. the target model (but does it technically increases the number of queries?)\n",
    "def get_label(input_text):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Query the target model\n",
    "    with torch.no_grad():\n",
    "        target_outputs = target_model(**inputs)\n",
    "\n",
    "    target_labels = target_outputs.logits.softmax(dim=1).tolist()[0]\n",
    "    return target_labels\n",
    "\n",
    "test_data['label'] = test_data['tweet'].apply(get_label) # ~ 2/3 mins\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-trained embeddings\n",
    "# Load pre-trained GloVe embeddings\n",
    "embed_dim = 100\n",
    "glove = vocab.GloVe(name='6B', dim=embed_dim)\n",
    "\n",
    "# Get the vocabulary from the pre-trained embeddings\n",
    "glove_vocab = glove.stoi  # Dictionary mapping words to their indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone Model class\n",
    "class HateSpeechGRU(nn.Module):\n",
    "    def __init__(self, pretrained_embeddings, hidden_size, output_dim, dropout):\n",
    "        super(HateSpeechGRU, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=True)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_size, num_layers=1, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_dim)  # * 2 for bidirectional\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)  # text: [batch size, sent len]\n",
    "        output, hidden = self.gru(embedded)  # output: [batch size, sent len, hidden_size * num_directions]\n",
    "        hidden = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)  # concatenate the final forward and backward hidden states\n",
    "        hidden = self.dropout(hidden)\n",
    "        output = self.fc(hidden)  # output: [batch size, output dim]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone Model\n",
    "# Define hyperparameters\n",
    "pretrained_embeddings = glove.vectors # Create a matrix of pre-trained embeddings\n",
    "hidden_size = 128  # Size of hidden states in the GRU\n",
    "output_dim = 2  # Number of output classes (binary classification)\n",
    "dropout = 0.5  # Dropout probability\n",
    "\n",
    "clone_model = HateSpeechGRU(pretrained_embeddings, hidden_size, output_dim, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @FunSizedYogi: @TheBlackVoice well how else...</td>\n",
       "      <td>[-0.04517167806625366, 0.13797102868556976, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Funny thing is....it's not just the people doi...</td>\n",
       "      <td>[0.05024722218513489, 0.0015787003794685006, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @winkSOSA: \"@AintShitSweet__: \"@Rakwon_OGOD...</td>\n",
       "      <td>[-0.11634157598018646, 0.04812651500105858, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Jbrendaro30 @ZGabrail @ramsin1995 @GabeEli8 @...</td>\n",
       "      <td>[-0.15736794471740723, -0.020282883197069168, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S/o that real bitch</td>\n",
       "      <td>[-0.11650463938713074, -0.026171239092946053, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19821</th>\n",
       "      <td>The last at-bat at Yankee Stadium. Thanks for ...</td>\n",
       "      <td>[-0.027169346809387207, 0.11435814201831818, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19822</th>\n",
       "      <td>@_bradleey LMFAOOOO yooo I lost my elevator pa...</td>\n",
       "      <td>[-0.028217773884534836, -0.05430904030799866, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19823</th>\n",
       "      <td>#porn,#android,#iphone,#ipad,#sex,#xxx, | #Ana...</td>\n",
       "      <td>[0.020860467106103897, -0.0533132441341877, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19824</th>\n",
       "      <td>RT @JennyJohnsonHi5: Just when I thought Justi...</td>\n",
       "      <td>[0.009069086983799934, -0.011038362048566341, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19825</th>\n",
       "      <td>bitches ain&amp;#8217;t shit, and they ain&amp;#8217;t...</td>\n",
       "      <td>[-0.06774534285068512, -0.016362417489290237, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19826 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   tweet  \\\n",
       "0      RT @FunSizedYogi: @TheBlackVoice well how else...   \n",
       "1      Funny thing is....it's not just the people doi...   \n",
       "2      RT @winkSOSA: \"@AintShitSweet__: \"@Rakwon_OGOD...   \n",
       "3      @Jbrendaro30 @ZGabrail @ramsin1995 @GabeEli8 @...   \n",
       "4                                    S/o that real bitch   \n",
       "...                                                  ...   \n",
       "19821  The last at-bat at Yankee Stadium. Thanks for ...   \n",
       "19822  @_bradleey LMFAOOOO yooo I lost my elevator pa...   \n",
       "19823  #porn,#android,#iphone,#ipad,#sex,#xxx, | #Ana...   \n",
       "19824  RT @JennyJohnsonHi5: Just when I thought Justi...   \n",
       "19825  bitches ain&#8217;t shit, and they ain&#8217;t...   \n",
       "\n",
       "                                               embedding  \n",
       "0      [-0.04517167806625366, 0.13797102868556976, 0....  \n",
       "1      [0.05024722218513489, 0.0015787003794685006, 0...  \n",
       "2      [-0.11634157598018646, 0.04812651500105858, 0....  \n",
       "3      [-0.15736794471740723, -0.020282883197069168, ...  \n",
       "4      [-0.11650463938713074, -0.026171239092946053, ...  \n",
       "...                                                  ...  \n",
       "19821  [-0.027169346809387207, 0.11435814201831818, 0...  \n",
       "19822  [-0.028217773884534836, -0.05430904030799866, ...  \n",
       "19823  [0.020860467106103897, -0.0533132441341877, 0....  \n",
       "19824  [0.009069086983799934, -0.011038362048566341, ...  \n",
       "19825  [-0.06774534285068512, -0.016362417489290237, ...  \n",
       "\n",
       "[19826 rows x 2 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-trained Sentence Transformer model\n",
    "sent_transformer = SentenceTransformer(\"all-MiniLM-L6-v2\").to(device)\n",
    "\n",
    "# Create the embeddings for the training data\n",
    "# ~ 30 secs\n",
    "train_embeddings = sent_transformer.encode(train_data['tweet'].tolist(), convert_to_tensor=True).tolist()\n",
    "\n",
    "#normalize the embeddings\n",
    "train_embeddings = torch.tensor(train_embeddings).to(device)\n",
    "train_embeddings = (train_embeddings - torch.min(train_embeddings, dim=0).values) / (torch.max(train_embeddings, dim=0).values - torch.min(train_embeddings, dim=0).values)\n",
    "print(train_embeddings.shape)\n",
    "# print(train_embeddings.shape)\n",
    "\n",
    "# train_data['embedding'] = train_embeddings.cpu().tolist()\n",
    "\n",
    "train_data['embedding'] = train_embeddings\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([19826, 384])\n",
      "torch.Size([19826, 384])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @FunSizedYogi: @TheBlackVoice well how else...</td>\n",
       "      <td>[-0.04517167806625366, 0.13797102868556976, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Funny thing is....it's not just the people doi...</td>\n",
       "      <td>[0.05024722218513489, 0.0015787003794685006, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @winkSOSA: \"@AintShitSweet__: \"@Rakwon_OGOD...</td>\n",
       "      <td>[-0.11634157598018646, 0.04812651500105858, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Jbrendaro30 @ZGabrail @ramsin1995 @GabeEli8 @...</td>\n",
       "      <td>[-0.15736794471740723, -0.020282883197069168, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S/o that real bitch</td>\n",
       "      <td>[-0.11650464683771133, -0.026171240955591202, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19821</th>\n",
       "      <td>The last at-bat at Yankee Stadium. Thanks for ...</td>\n",
       "      <td>[-0.027169346809387207, 0.11435814201831818, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19822</th>\n",
       "      <td>@_bradleey LMFAOOOO yooo I lost my elevator pa...</td>\n",
       "      <td>[-0.028217773884534836, -0.05430904030799866, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19823</th>\n",
       "      <td>#porn,#android,#iphone,#ipad,#sex,#xxx, | #Ana...</td>\n",
       "      <td>[0.020860467106103897, -0.0533132441341877, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19824</th>\n",
       "      <td>RT @JennyJohnsonHi5: Just when I thought Justi...</td>\n",
       "      <td>[0.009069086983799934, -0.011038362048566341, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19825</th>\n",
       "      <td>bitches ain&amp;#8217;t shit, and they ain&amp;#8217;t...</td>\n",
       "      <td>[-0.06774534285068512, -0.016362417489290237, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19826 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   tweet  \\\n",
       "0      RT @FunSizedYogi: @TheBlackVoice well how else...   \n",
       "1      Funny thing is....it's not just the people doi...   \n",
       "2      RT @winkSOSA: \"@AintShitSweet__: \"@Rakwon_OGOD...   \n",
       "3      @Jbrendaro30 @ZGabrail @ramsin1995 @GabeEli8 @...   \n",
       "4                                    S/o that real bitch   \n",
       "...                                                  ...   \n",
       "19821  The last at-bat at Yankee Stadium. Thanks for ...   \n",
       "19822  @_bradleey LMFAOOOO yooo I lost my elevator pa...   \n",
       "19823  #porn,#android,#iphone,#ipad,#sex,#xxx, | #Ana...   \n",
       "19824  RT @JennyJohnsonHi5: Just when I thought Justi...   \n",
       "19825  bitches ain&#8217;t shit, and they ain&#8217;t...   \n",
       "\n",
       "                                               embedding  \n",
       "0      [-0.04517167806625366, 0.13797102868556976, 0....  \n",
       "1      [0.05024722218513489, 0.0015787003794685006, 0...  \n",
       "2      [-0.11634157598018646, 0.04812651500105858, 0....  \n",
       "3      [-0.15736794471740723, -0.020282883197069168, ...  \n",
       "4      [-0.11650464683771133, -0.026171240955591202, ...  \n",
       "...                                                  ...  \n",
       "19821  [-0.027169346809387207, 0.11435814201831818, 0...  \n",
       "19822  [-0.028217773884534836, -0.05430904030799866, ...  \n",
       "19823  [0.020860467106103897, -0.0533132441341877, 0....  \n",
       "19824  [0.009069086983799934, -0.011038362048566341, ...  \n",
       "19825  [-0.06774534285068512, -0.016362417489290237, ...  \n",
       "\n",
       "[19826 rows x 2 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-trained Sentence Transformer model\n",
    "sent_transformer = SentenceTransformer(\"all-MiniLM-L6-v2\").to(device)\n",
    "\n",
    "# Create the embeddings for the training data\n",
    "# ~ 30 secs\n",
    "train_embeddings = sent_transformer.encode(train_data['tweet'].tolist(), convert_to_tensor=True).tolist()\n",
    "\n",
    "#normalize the embeddings\n",
    "train_embeddings = torch.tensor(train_embeddings).to(device)\n",
    "print(train_embeddings.shape)\n",
    "train_embeddings = nn.functional.normalize(train_embeddings, p=2, dim=1)\n",
    "print(train_embeddings.shape)\n",
    "\n",
    "train_data['embedding'] = train_embeddings.cpu().tolist()\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "girdigi sayi 0\n",
      "Epoch [1], Loss: 3.887749433517456\n",
      "table size  (128, 5)\n",
      "train_data size  (128,)\n",
      "girdigi sayi 0\n",
      "Epoch [2], Loss: 3.864238977432251\n",
      "table size  (256, 5)\n",
      "train_data size  (128,)\n",
      "girdigi sayi 0\n",
      "Epoch [3], Loss: 3.8418831825256348\n",
      "table size  (384, 5)\n",
      "train_data size  (128,)\n",
      "girdigi sayi 0\n",
      "Epoch [4], Loss: 3.820136785507202\n",
      "table size  (512, 5)\n",
      "train_data size  (128,)\n",
      "girdigi sayi 0\n",
      "Epoch [5], Loss: 3.7993886470794678\n",
      "table size  (640, 5)\n",
      "train_data size  (128,)\n",
      "girdigi sayi 0\n",
      "Epoch [6], Loss: 3.7764151096343994\n",
      "table size  (768, 5)\n",
      "train_data size  (128,)\n",
      "girdigi sayi 0\n",
      "Epoch [7], Loss: 3.755317449569702\n",
      "table size  (896, 5)\n",
      "train_data size  (128,)\n",
      "girdigi sayi 0\n",
      "Epoch [8], Loss: 3.7334978580474854\n",
      "table size  (1024, 5)\n",
      "train_data size  (128,)\n",
      "girdigi sayi 0\n",
      "Epoch [9], Loss: 3.7133593559265137\n",
      "table size  (1152, 5)\n",
      "train_data size  (128,)\n",
      "girdigi sayi 0\n",
      "Epoch [10], Loss: 3.69266939163208\n",
      "table size  (1280, 5)\n",
      "train_data size  (128,)\n",
      "girdigi sayi 0\n",
      "Epoch [11], Loss: 3.6716384887695312\n",
      "table size  (1408, 5)\n",
      "train_data size  (128,)\n",
      "girdigi sayi 0\n",
      "Epoch [12], Loss: 3.650346040725708\n",
      "table size  (1536, 5)\n",
      "train_data size  (128,)\n",
      "girdigi sayi 0\n",
      "Epoch [13], Loss: 3.6304547786712646\n",
      "table size  (1664, 5)\n",
      "train_data size  (128,)\n",
      "girdigi sayi 0\n",
      "Epoch [14], Loss: 3.61089825630188\n",
      "table size  (1792, 5)\n",
      "train_data size  (128,)\n",
      "girdigi sayi 0\n",
      "Epoch [15], Loss: 3.591277837753296\n",
      "table size  (1920, 5)\n",
      "train_data size  (128,)\n",
      "girdigi sayi 0\n",
      "Epoch [16], Loss: 3.569779634475708\n",
      "table size  (2048, 5)\n",
      "train_data size  (128,)\n",
      "girdigi sayi 0\n",
      "Epoch [17], Loss: 3.54794979095459\n",
      "table size  (2176, 5)\n",
      "train_data size  (128,)\n",
      "girdigi sayi 0\n",
      "Epoch [18], Loss: 3.5256521701812744\n",
      "table size  (2304, 5)\n",
      "train_data size  (128,)\n",
      "girdigi sayi 0\n",
      "Epoch [19], Loss: 3.5006422996520996\n",
      "table size  (2432, 5)\n",
      "train_data size  (128,)\n",
      "girdigi sayi 0\n",
      "Epoch [20], Loss: 3.4733734130859375\n",
      "table size  (2560, 5)\n",
      "train_data size  (128,)\n",
      "girdigi sayi 0\n",
      "Epoch [21], Loss: 3.441621780395508\n",
      "table size  (2688, 5)\n",
      "train_data size  (128,)\n",
      "girdigi sayi 0\n",
      "Epoch [22], Loss: 3.4088354110717773\n",
      "table size  (2816, 5)\n",
      "train_data size  (128,)\n",
      "girdigi sayi 0\n",
      "Epoch [23], Loss: 3.376537322998047\n",
      "table size  (2944, 5)\n",
      "train_data size  (128,)\n",
      "girdigi sayi 0\n",
      "Epoch [24], Loss: 3.3425400257110596\n",
      "table size  (3072, 5)\n",
      "train_data size  (128,)\n",
      "girdigi sayi 0\n",
      "Epoch [25], Loss: 3.307385206222534\n",
      "table size  (3200, 5)\n",
      "train_data size  (128,)\n",
      "girdigi sayi 0\n",
      "Epoch [26], Loss: 3.271583080291748\n",
      "table size  (3328, 5)\n",
      "train_data size  (128,)\n",
      "girdigi sayi 0\n",
      "Epoch [27], Loss: 3.233384132385254\n",
      "table size  (3456, 5)\n",
      "train_data size  (128,)\n",
      "girdigi sayi 0\n",
      "Epoch [28], Loss: 3.1938376426696777\n",
      "table size  (3584, 5)\n",
      "train_data size  (128,)\n",
      "girdigi sayi 0\n",
      "Epoch [29], Loss: 3.148890256881714\n",
      "table size  (3712, 5)\n",
      "train_data size  (128,)\n",
      "girdigi sayi 0\n",
      "Epoch [30], Loss: 3.0965585708618164\n",
      "table size  (3840, 5)\n",
      "train_data size  (128,)\n",
      "girdigi sayi 0\n",
      "Epoch [31], Loss: 3.0339300632476807\n",
      "table size  (3968, 5)\n",
      "train_data size  (128,)\n",
      "girdigi sayi 0\n",
      "Epoch [32], Loss: 2.9707159996032715\n",
      "table size  (4096, 5)\n",
      "train_data size  (128,)\n",
      "girdigi sayi 0\n",
      "Epoch [33], Loss: 2.8996036052703857\n",
      "table size  (4224, 5)\n",
      "train_data size  (128,)\n",
      "girdigi sayi 0\n",
      "Epoch [34], Loss: 2.825094699859619\n",
      "table size  (4352, 5)\n",
      "train_data size  (128,)\n",
      "girdigi sayi 0\n",
      "Epoch [35], Loss: 2.7397239208221436\n",
      "table size  (4480, 5)\n",
      "train_data size  (128,)\n",
      "girdigi sayi 0\n",
      "Epoch [36], Loss: 2.658134698867798\n",
      "table size  (4608, 5)\n",
      "train_data size  (128,)\n",
      "girdigi sayi 0\n",
      "Epoch [37], Loss: 2.5684621334075928\n",
      "table size  (4736, 5)\n",
      "train_data size  (128,)\n",
      "girdigi sayi 0\n",
      "Epoch [38], Loss: 2.478466749191284\n",
      "table size  (4864, 5)\n",
      "train_data size  (128,)\n",
      "girdigi sayi 0\n",
      "Epoch [39], Loss: 2.4043874740600586\n",
      "table size  (4992, 5)\n",
      "train_data size  (128,)\n",
      "girdigi sayi 0\n",
      "Epoch [40], Loss: 2.3375542163848877\n",
      "table size  (5120, 5)\n",
      "train_data size  (128,)\n",
      "girdigi sayi 0\n",
      "Epoch [41], Loss: 2.2824347019195557\n",
      "table size  (5248, 5)\n",
      "train_data size  (128,)\n",
      "girdigi sayi 0\n",
      "Epoch [42], Loss: 2.2388150691986084\n",
      "table size  (5376, 5)\n",
      "train_data size  (128,)\n",
      "girdigi sayi 0\n",
      "Epoch [43], Loss: 2.200643539428711\n",
      "table size  (5504, 5)\n",
      "train_data size  (128,)\n",
      "girdigi sayi 0\n",
      "Epoch [44], Loss: 2.1674509048461914\n",
      "table size  (5632, 5)\n",
      "train_data size  (128,)\n",
      "girdigi sayi 1\n",
      "Epoch [45], Loss: 2.135694742202759\n",
      "table size  (5759, 5)\n",
      "train_data size  (127,)\n",
      "girdigi sayi 1\n",
      "Epoch [46], Loss: 2.107203483581543\n",
      "table size  (5886, 5)\n",
      "train_data size  (127,)\n",
      "girdigi sayi 1\n",
      "Epoch [47], Loss: 2.073251724243164\n",
      "table size  (6013, 5)\n",
      "train_data size  (127,)\n",
      "girdigi sayi 3\n",
      "Epoch [48], Loss: 2.0363929271698\n",
      "table size  (6138, 5)\n",
      "train_data size  (125,)\n",
      "girdigi sayi 5\n",
      "Epoch [49], Loss: 2.000901460647583\n",
      "table size  (6261, 5)\n",
      "train_data size  (123,)\n",
      "girdigi sayi 5\n",
      "Epoch [50], Loss: 1.9659401178359985\n",
      "table size  (6384, 5)\n",
      "train_data size  (123,)\n",
      "girdigi sayi 6\n",
      "Epoch [51], Loss: 1.9279165267944336\n",
      "table size  (6506, 5)\n",
      "train_data size  (122,)\n",
      "girdigi sayi 6\n",
      "Epoch [52], Loss: 1.8891832828521729\n",
      "table size  (6628, 5)\n",
      "train_data size  (122,)\n",
      "girdigi sayi 6\n",
      "Epoch [53], Loss: 1.8471635580062866\n",
      "table size  (6750, 5)\n",
      "train_data size  (122,)\n",
      "girdigi sayi 7\n",
      "Epoch [54], Loss: 1.800687313079834\n",
      "table size  (6871, 5)\n",
      "train_data size  (121,)\n",
      "girdigi sayi 9\n",
      "Epoch [55], Loss: 1.7531442642211914\n",
      "table size  (6990, 5)\n",
      "train_data size  (119,)\n",
      "girdigi sayi 11\n",
      "Epoch [56], Loss: 1.7101596593856812\n",
      "table size  (7107, 5)\n",
      "train_data size  (117,)\n",
      "girdigi sayi 11\n",
      "Epoch [57], Loss: 1.669972538948059\n",
      "table size  (7224, 5)\n",
      "train_data size  (117,)\n",
      "girdigi sayi 16\n",
      "Epoch [58], Loss: 1.6311018466949463\n",
      "table size  (7336, 5)\n",
      "train_data size  (112,)\n",
      "girdigi sayi 19\n",
      "Epoch [59], Loss: 1.587515115737915\n",
      "table size  (7445, 5)\n",
      "train_data size  (109,)\n",
      "girdigi sayi 20\n",
      "Epoch [60], Loss: 1.5400062799453735\n",
      "table size  (7553, 5)\n",
      "train_data size  (108,)\n",
      "girdigi sayi 24\n",
      "Epoch [61], Loss: 1.4962995052337646\n",
      "table size  (7657, 5)\n",
      "train_data size  (104,)\n",
      "girdigi sayi 34\n",
      "Epoch [62], Loss: 1.4580100774765015\n",
      "table size  (7751, 5)\n",
      "train_data size  (94,)\n",
      "girdigi sayi 39\n",
      "Epoch [63], Loss: 1.4321900606155396\n",
      "table size  (7840, 5)\n",
      "train_data size  (89,)\n",
      "girdigi sayi 43\n",
      "Epoch [64], Loss: 1.4079838991165161\n",
      "table size  (7925, 5)\n",
      "train_data size  (85,)\n",
      "girdigi sayi 48\n",
      "Epoch [65], Loss: 1.384602427482605\n",
      "table size  (8005, 5)\n",
      "train_data size  (80,)\n",
      "girdigi sayi 58\n",
      "Epoch [66], Loss: 1.3635047674179077\n",
      "table size  (8075, 5)\n",
      "train_data size  (70,)\n",
      "girdigi sayi 61\n",
      "Epoch [67], Loss: 1.3419851064682007\n",
      "table size  (8142, 5)\n",
      "train_data size  (67,)\n",
      "girdigi sayi 65\n",
      "Epoch [68], Loss: 1.3216075897216797\n",
      "table size  (8205, 5)\n",
      "train_data size  (63,)\n",
      "girdigi sayi 76\n",
      "Epoch [69], Loss: 1.3031189441680908\n",
      "table size  (8257, 5)\n",
      "train_data size  (52,)\n",
      "girdigi sayi 80\n",
      "Epoch [70], Loss: 1.286199927330017\n",
      "table size  (8305, 5)\n",
      "train_data size  (48,)\n",
      "girdigi sayi 88\n",
      "Epoch [71], Loss: 1.2708591222763062\n",
      "table size  (8345, 5)\n",
      "train_data size  (40,)\n",
      "girdigi sayi 92\n",
      "Epoch [72], Loss: 1.2587190866470337\n",
      "table size  (8381, 5)\n",
      "train_data size  (36,)\n",
      "girdigi sayi 94\n",
      "Epoch [73], Loss: 1.2470968961715698\n",
      "table size  (8415, 5)\n",
      "train_data size  (34,)\n",
      "girdigi sayi 97\n",
      "Epoch [74], Loss: 1.2382162809371948\n",
      "table size  (8446, 5)\n",
      "train_data size  (31,)\n",
      "girdigi sayi 102\n",
      "Epoch [75], Loss: 1.2316514253616333\n",
      "table size  (8472, 5)\n",
      "train_data size  (26,)\n",
      "girdigi sayi 105\n",
      "Epoch [76], Loss: 1.2261286973953247\n",
      "table size  (8495, 5)\n",
      "train_data size  (23,)\n",
      "girdigi sayi 107\n",
      "Epoch [77], Loss: 1.2208654880523682\n",
      "table size  (8516, 5)\n",
      "train_data size  (21,)\n",
      "girdigi sayi 110\n",
      "Epoch [78], Loss: 1.215811014175415\n",
      "table size  (8534, 5)\n",
      "train_data size  (18,)\n",
      "girdigi sayi 110\n",
      "Epoch [79], Loss: 1.2122774124145508\n",
      "table size  (8552, 5)\n",
      "train_data size  (18,)\n",
      "girdigi sayi 112\n",
      "Epoch [80], Loss: 1.2097678184509277\n",
      "table size  (8568, 5)\n",
      "train_data size  (16,)\n",
      "girdigi sayi 114\n",
      "Epoch [81], Loss: 1.2093310356140137\n",
      "table size  (8582, 5)\n",
      "train_data size  (14,)\n",
      "girdigi sayi 114\n",
      "Epoch [82], Loss: 1.2084124088287354\n",
      "table size  (8596, 5)\n",
      "train_data size  (14,)\n",
      "girdigi sayi 114\n",
      "Epoch [83], Loss: 1.2071683406829834\n",
      "table size  (8610, 5)\n",
      "train_data size  (14,)\n",
      "girdigi sayi 115\n",
      "Epoch [84], Loss: 1.2046791315078735\n",
      "table size  (8623, 5)\n",
      "train_data size  (13,)\n",
      "girdigi sayi 117\n",
      "Epoch [85], Loss: 1.205639362335205\n",
      "table size  (8634, 5)\n",
      "train_data size  (11,)\n",
      "girdigi sayi 121\n",
      "Epoch [86], Loss: 1.2086660861968994\n",
      "table size  (8641, 5)\n",
      "train_data size  (7,)\n",
      "girdigi sayi 123\n",
      "Epoch [87], Loss: 1.2109369039535522\n",
      "table size  (8646, 5)\n",
      "train_data size  (5,)\n",
      "girdigi sayi 123\n",
      "Epoch [88], Loss: 1.213375210762024\n",
      "table size  (8651, 5)\n",
      "train_data size  (5,)\n",
      "girdigi sayi 123\n",
      "Epoch [89], Loss: 1.216429591178894\n",
      "table size  (8656, 5)\n",
      "train_data size  (5,)\n",
      "girdigi sayi 123\n",
      "Epoch [90], Loss: 1.2200058698654175\n",
      "table size  (8661, 5)\n",
      "train_data size  (5,)\n",
      "girdigi sayi 124\n",
      "Epoch [91], Loss: 1.2249037027359009\n",
      "table size  (8665, 5)\n",
      "train_data size  (4,)\n",
      "girdigi sayi 124\n",
      "Epoch [92], Loss: 1.232745885848999\n",
      "table size  (8669, 5)\n",
      "train_data size  (4,)\n",
      "girdigi sayi 123\n",
      "Epoch [93], Loss: 1.2368099689483643\n",
      "table size  (8674, 5)\n",
      "train_data size  (5,)\n",
      "girdigi sayi 123\n",
      "Epoch [94], Loss: 1.238123893737793\n",
      "table size  (8679, 5)\n",
      "train_data size  (5,)\n",
      "girdigi sayi 123\n",
      "Epoch [95], Loss: 1.2371776103973389\n",
      "table size  (8684, 5)\n",
      "train_data size  (5,)\n",
      "girdigi sayi 125\n",
      "Epoch [96], Loss: 1.2395596504211426\n",
      "table size  (8687, 5)\n",
      "train_data size  (3,)\n",
      "girdigi sayi 125\n",
      "Epoch [97], Loss: 1.2427817583084106\n",
      "table size  (8690, 5)\n",
      "train_data size  (3,)\n",
      "girdigi sayi 125\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[82], line 128\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    126\u001b[0m     target_outputs \u001b[38;5;241m=\u001b[39m target_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minput_tokens)\n\u001b[0;32m--> 128\u001b[0m target_labels \u001b[38;5;241m=\u001b[39m \u001b[43mtarget_outputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m# Conver input_text to tensor using glove_vocab\u001b[39;00m\n\u001b[1;32m    131\u001b[0m input_glove \u001b[38;5;241m=\u001b[39m [[glove_vocab[word] \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m glove_vocab \u001b[38;5;28;01melse\u001b[39;00m glove_vocab[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munk\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m input_text\u001b[38;5;241m.\u001b[39msplit()] \u001b[38;5;28;01mfor\u001b[39;00m input_text \u001b[38;5;129;01min\u001b[39;00m input_texts]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Table to store the previous queries (for wise query selection later)\n",
    "table = pd.DataFrame({\n",
    "    'tweet': [],\n",
    "    'embedding': [],\n",
    "    't_out': [],\n",
    "    'c_out': [],\n",
    "    'count': []\n",
    "})\n",
    "\n",
    "alpha = 0.5 # Weight for 'dissimilariy with the previous queries' term from the formula\n",
    "beta = 1-alpha # Weight for 'similarity with the previous queries that had hish disagreement' term from the formula\n",
    "\n",
    "\n",
    "# Train the clone model\n",
    "# Define loss function and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(clone_model.parameters(), lr=0.0001)\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "# Define number of epochs\n",
    "epoch = 0\n",
    "\n",
    "loss_vals = []\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Training loop\n",
    "    \n",
    "while True:\n",
    "    clone_model.train()\n",
    "    \n",
    "    if table.shape[0] == 0: # Cold start for the first query (can be random)\n",
    "        idxs = torch.randperm(len(train_data))[:batch_size].tolist()\n",
    "    else:\n",
    "        # Wise query selection\n",
    "        # Calculate the cosine similarity between the embeddings of the training data and the table\n",
    "        similarities = util.cos_sim(torch.tensor(train_data['embedding'].tolist()), torch.tensor(table['embedding'].tolist()))\n",
    "        \n",
    "        # Calculate the average cosine similarity for each training data\n",
    "        # 'dissimilariy with the previous queries'\n",
    "        avg_similarities = similarities.max(dim=1).values\n",
    "\n",
    "        # 'similarity with the previous queries that had hish disagreement'\n",
    "        disagreement = torch.tensor((abs(table['c_out'] - table['t_out'])).tolist())\n",
    "        \n",
    "        similarities = similarities * disagreement\n",
    "        avg_wrt_disagreement = similarities.mean(dim=1)\n",
    "\n",
    "        # Calculating Z-scores for avg_similarities and avg_wrt_disagreement\n",
    "        avg_similarities_z = (avg_similarities - avg_similarities.mean()) / avg_similarities.std()\n",
    "        avg_wrt_disagreement_z = (avg_wrt_disagreement - avg_wrt_disagreement.mean()) / avg_wrt_disagreement.std()\n",
    "\n",
    "        # Calculate the normalized formula for each training data\n",
    "        formula = alpha * (-avg_similarities_z) + beta * avg_wrt_disagreement_z\n",
    "        \n",
    "        # Get the index of the training data with the lowest average cosine similarity\n",
    "        idxs = formula.argsort(descending=True)[:batch_size].tolist()\n",
    "\n",
    "        #print('argsorted formula', formula.sort(descending=True)[:batch_size]) \n",
    "    # else:\n",
    "    #     # Wise query selection\n",
    "    #     # Calculate the cosine similarity between the embeddings of the training data and the table\n",
    "    #     similarities = util.cos_sim(torch.tensor(train_data['embedding'].tolist()), torch.tensor(table['embedding'].tolist()))\n",
    "    #     # calculate the euclidian distance between embeddings\n",
    "    #     # similarities = -torch.cdist(torch.tensor(train_data['embedding'].tolist()), torch.tensor(table['embedding'].tolist()), p=1)\n",
    "    #     print('similarity shape', similarities.shape)\n",
    "    #     # Calculate the average cosine similarity for each training data\n",
    "    #     # 'dissimilariy with the previous queries'\n",
    "\n",
    "    #     #count_vec = torch.tensor(table['count'].tolist())\n",
    "    #     # raise count_vec to power of e\n",
    "    #     #count_vec = torch.exp(count_vec)\n",
    "    #     #avg_similarities = (similarities @ count_vec) / table.shape[0]\n",
    "    #     #print('table shape', table.shape[0])\n",
    "\n",
    "\n",
    "    #     #avg_similarities = similarities.mean(dim=1)\n",
    "\n",
    "    #     # find the mean of the similarities with the previous \n",
    "\n",
    "    #     max_similarities = similarities.max(dim=1).values\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #     # 'similarity with the previous queries that had hish disagreement'\n",
    "    #     disagreement = torch.tensor((abs(table['c_out'] - table['t_out'])).tolist())\n",
    "        \n",
    "    #     similarities = similarities * disagreement\n",
    "    #     avg_wrt_disagreement = similarities.mean(dim=1)\n",
    "\n",
    "    #     # Calculate the formula for each training data\n",
    "    #     formula = alpha * (-max_similarities) + beta * avg_wrt_disagreement\n",
    "\n",
    "\n",
    "        \n",
    "    #     # Get the index of the training data with the lowest average cosine similarity\n",
    "    #     idxs = formula.argsort(descending=True)[:batch_size].tolist()\n",
    "\n",
    "\n",
    "    input_texts = train_data.iloc[idxs]['tweet']\n",
    "\n",
    "    # Check if the input_texts are already queried\n",
    "    will_added = [True] * len(input_texts)\n",
    "\n",
    "    count = 0\n",
    "    for t in range(len(input_texts)):\n",
    "        if input_texts.iloc[t] in table['tweet'].tolist():\n",
    "            will_added[t] = False\n",
    "            count += 1\n",
    "            table.loc[table['tweet'] == input_texts.iloc[t], 'count'] += 1\n",
    "        #print(type(input_texts.iloc[t]))\n",
    "    print('girdigi sayi', count)\n",
    "\n",
    "    \n",
    "            \n",
    "\n",
    "    input_embeds = train_data.iloc[idxs]['embedding']\n",
    "\n",
    "    # inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "    input_tokens = tokenizer(input_texts.tolist(), return_tensors=\"pt\", padding=True, truncation=False).to(device)\n",
    "\n",
    "    # Query the target model\n",
    "    with torch.no_grad():\n",
    "        target_outputs = target_model(**input_tokens)\n",
    "\n",
    "    target_labels = target_outputs.logits.softmax(dim=1).tolist()\n",
    "\n",
    "    # Conver input_text to tensor using glove_vocab\n",
    "    input_glove = [[glove_vocab[word] if word in glove_vocab else glove_vocab['unk'] for word in input_text.split()] for input_text in input_texts]\n",
    "    max_len = max([len(input_text) for input_text in input_glove])\n",
    "    input_glove = [input_text + [glove_vocab['unk']] * (max_len - len(input_text)) for input_text in input_glove]\n",
    "    input_glove = torch.tensor(input_glove, dtype=torch.long).to(device)\n",
    "\n",
    "    target_outputs = torch.tensor(target_labels).to(device)\n",
    "\n",
    "    # Forward pass through the clone model\n",
    "    with autocast():\n",
    "        clone_logits = clone_model(input_glove)\n",
    "        clone_outputs = torch.softmax(clone_logits, dim=-1)\n",
    "        loss = criterion(clone_outputs, target_outputs)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    scaler.scale(loss).backward()\n",
    "\n",
    "    # Update model weights considering the scaled gradients\n",
    "    scaler.step(optimizer)\n",
    "\n",
    "    # Updates the scale for next iteration\n",
    "    scaler.update()\n",
    "\n",
    "    # Remove the queried data from the training data\n",
    "    input_texts = input_texts[will_added]\n",
    "    input_embeds = input_embeds[will_added]\n",
    "    target_outputs = target_outputs[will_added]\n",
    "    clone_outputs = clone_outputs[will_added]\n",
    "\n",
    "    # Add row to table\n",
    "    row = pd.DataFrame({'tweet': input_texts, 'embedding': input_embeds, 't_out': target_outputs[:,0].tolist(), 'c_out': clone_outputs[:,0].tolist(), 'count': [1] * len(input_texts)})\n",
    "    table = pd.concat([table, row], ignore_index=True)\n",
    "\n",
    "    # Evaluation\n",
    "    clone_model.eval()\n",
    "\n",
    "    with torch.inference_mode():\n",
    "\n",
    "        # Updating the table\n",
    "        input_glove_2 = [[glove_vocab[word] if word in glove_vocab else glove_vocab['unk'] for word in input_text.split()] for input_text in table['tweet'].tolist()]\n",
    "        max_len = max([len(input_text) for input_text in input_glove_2])\n",
    "        input_glove_2 = [input_text + [glove_vocab['unk']] * (max_len - len(input_text)) for input_text in input_glove_2]\n",
    "        input_glove_2 = torch.tensor(input_glove_2, dtype=torch.long).to(device)\n",
    "\n",
    "        with autocast():\n",
    "            clone_logits_2 = clone_model(input_glove_2)\n",
    "            clone_outputs_2 = torch.softmax(clone_logits_2, dim=-1)\n",
    "\n",
    "        table['c_out'] = clone_outputs_2[:,0].tolist()\n",
    "        \n",
    "        # Validation set\n",
    "        inputs = [[glove_vocab[word] if word in glove_vocab else glove_vocab['unk'] for word in txt.split()] for txt in test_data['tweet'].tolist()] # TODO: Do once, don't do it in every epoch\n",
    "        max_len = max([len(txt) for txt in inputs])\n",
    "        inputs = [txt + [glove_vocab['unk']] * (max_len - len(txt)) for txt in inputs]\n",
    "        test_pred = clone_model(torch.tensor(inputs, device=device))\n",
    "        test_loss = criterion(test_pred, torch.tensor(test_data['label'].tolist(), device=device))\n",
    "\n",
    "    loss_vals.append(test_loss.item())\n",
    "    print(f'Epoch [{epoch + 1}], Loss: {test_loss.item()}')\n",
    "\n",
    "    epoch += 1\n",
    "\n",
    "    print('table size ' ,table.shape)\n",
    "    print('train_data size ' ,input_texts.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>embedding</th>\n",
       "      <th>t_out</th>\n",
       "      <th>c_out</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is my \"sup bitches\" face http://t.co/mqPq...</td>\n",
       "      <td>[-0.1008186861872673, 0.006707882042974234, 0....</td>\n",
       "      <td>0.522070</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Two days gone she been gone so long you can't ...</td>\n",
       "      <td>[-0.10198283940553665, -0.05068354308605194, 0...</td>\n",
       "      <td>0.214770</td>\n",
       "      <td>0.999991</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#Yankees #FireCashman I don't want Arod back.</td>\n",
       "      <td>[-0.025286220014095306, -0.03454281762242317, ...</td>\n",
       "      <td>0.368439</td>\n",
       "      <td>0.999994</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&amp;#8220;@lifeof_brandon: @100046729 we'll be to...</td>\n",
       "      <td>[-0.13836750388145447, 0.005380598362535238, 0...</td>\n",
       "      <td>0.127643</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @ThatKidJony: I'm trying to pass my nigguh....</td>\n",
       "      <td>[-0.09449616074562073, 0.05206477642059326, 0....</td>\n",
       "      <td>0.101626</td>\n",
       "      <td>0.999992</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>@ohVixen I love laying in bed by myself becaus...</td>\n",
       "      <td>[0.0153201287612319, -0.0395226776599884, 0.03...</td>\n",
       "      <td>0.889367</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>#NP \"Centuries\" ~FOB</td>\n",
       "      <td>[-0.0013627943117171526, 0.09317202121019363, ...</td>\n",
       "      <td>0.662812</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>yay I like when jacob has colored layouts</td>\n",
       "      <td>[-0.10147412866353989, 0.006389786023646593, 0...</td>\n",
       "      <td>0.973427</td>\n",
       "      <td>0.999873</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>I know how it feels to not have shit, believe ...</td>\n",
       "      <td>[0.07379258424043655, -0.02337886579334736, 0....</td>\n",
       "      <td>0.962512</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Who takes mid from the best midlaner NA??? Fuc...</td>\n",
       "      <td>[-0.006915649864822626, 0.025266606360673904, ...</td>\n",
       "      <td>0.103457</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>96 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                tweet  \\\n",
       "0   This is my \"sup bitches\" face http://t.co/mqPq...   \n",
       "1   Two days gone she been gone so long you can't ...   \n",
       "2       #Yankees #FireCashman I don't want Arod back.   \n",
       "3   &#8220;@lifeof_brandon: @100046729 we'll be to...   \n",
       "4   RT @ThatKidJony: I'm trying to pass my nigguh....   \n",
       "..                                                ...   \n",
       "91  @ohVixen I love laying in bed by myself becaus...   \n",
       "92                               #NP \"Centuries\" ~FOB   \n",
       "93          yay I like when jacob has colored layouts   \n",
       "94  I know how it feels to not have shit, believe ...   \n",
       "95  Who takes mid from the best midlaner NA??? Fuc...   \n",
       "\n",
       "                                            embedding     t_out     c_out  \\\n",
       "0   [-0.1008186861872673, 0.006707882042974234, 0....  0.522070  0.999997   \n",
       "1   [-0.10198283940553665, -0.05068354308605194, 0...  0.214770  0.999991   \n",
       "2   [-0.025286220014095306, -0.03454281762242317, ...  0.368439  0.999994   \n",
       "3   [-0.13836750388145447, 0.005380598362535238, 0...  0.127643  0.999996   \n",
       "4   [-0.09449616074562073, 0.05206477642059326, 0....  0.101626  0.999992   \n",
       "..                                                ...       ...       ...   \n",
       "91  [0.0153201287612319, -0.0395226776599884, 0.03...  0.889367  0.999998   \n",
       "92  [-0.0013627943117171526, 0.09317202121019363, ...  0.662812  0.999993   \n",
       "93  [-0.10147412866353989, 0.006389786023646593, 0...  0.973427  0.999873   \n",
       "94  [0.07379258424043655, -0.02337886579334736, 0....  0.962512  0.999999   \n",
       "95  [-0.006915649864822626, 0.025266606360673904, ...  0.103457  0.999996   \n",
       "\n",
       "    count  \n",
       "0     1.0  \n",
       "1     1.0  \n",
       "2     1.0  \n",
       "3     1.0  \n",
       "4     1.0  \n",
       "..    ...  \n",
       "91    1.0  \n",
       "92    1.0  \n",
       "93    1.0  \n",
       "94    1.0  \n",
       "95    1.0  \n",
       "\n",
       "[96 rows x 5 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take the unique tweets of table\n",
    "table2 = table.drop_duplicates(subset='tweet', keep='first')\n",
    "table2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
